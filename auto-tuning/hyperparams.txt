#####################################################################
# Hyperparameters for preprocess-bpe.sh, train.sh and auto-tuning   #
#                                                                   #
# Overview:                                                         #
# - "workdir" corresponds a group of preprocessed bitext and models #
#    for a given dataset. Each "workdir" can contain multiple       #
#    "datadir" and "modeldir" if desired                            #
# - "datadir" stores the BPE-preprocessed training and validation   #
#    bitext files                                                   #
# - "modeldir" is generated by Sockeye and stores all training info #
# - "rootdir" is path to your installation of sockeye-recipes,      #
#    e.g. ~/src/sockeye-recipes                                     #
# - "autotune_dir" is path to auto-tuning scripts.                  #
#                                                                   #
# preprocess-bpe.sh:                                                #
# - input: Tokenized bitext for training ("train_tok") and          #
#   and validation ("valid_tok")                                    #
# - output: BPE-preprocessed bitext ("train_bpe", "valid_bpe")      #
#   and vocabulary ("bpe_vocab_src", "bpe_vocab_trg")               #
# - main hyperparameters: number of BPE symbols for source & target #
#                                                                   #
# train.sh:                                                         #
# - input: BPE-preprocessed bitext ("train_bpe", "valid_bpe")       #
# - output: "modeldir", which contains all training info and can    #
#    be used to translate                                           #
# - main hyperparameters: many! see below                           #
#####################################################################

# export all variables
set -a 

#####################################################################
# (0) General settings (to be modified for each project)            #
#####################################################################

### User-specified directories ###
workdir=~/sockeye_trial_dev/
datadir=$workdir/data
rootdir=~/my-sockeye-recipes/
autotunedir=$rootdir/auto-tuning/

### Language pair (source and target) ###
# Note: We assume all bitext files contain these as suffices. 
# e.g. $train_tok.$src, $train_tok.$trg refer to the source and target 
src=zh
trg=en

### Tokenized training and validation data ###
# Note we assume tokenization is already done, and will only run BPE
# For tokenization and other preprocessing, see preprocess-tokenize.sh
# which does not use this hyperparam.txt file
train_tok=$workdir/sample-de-en/train
valid_tok=$workdir/sample-de-en/valid



#####################################################################
# (1) preprocess-bpe.sh settings (modify if needed)                 #
#####################################################################

### Number of symbols to use for BPE ###
# Note: we perform source and target BPE separately
# This corresponds to initial source (src) and target (trg) vocab size
bpe_symbols_src=4000
bpe_symbols_trg=4000

### Filename for BPE-processed bitext file ###
# Note: the following default names should be fine for most use cases
train_bpe=$datadir/train.bpe-${bpe_symbols_src}
valid_bpe=$datadir/valid.bpe-${bpe_symbols_src}

### Filename for BPE vocabulary ###
# Note: the following default names should be fine for most use cases
# Note: bpe_vocab_src will be needed for applying BPE to test, in translate.sh
bpe_vocab_src=${train_bpe}.$src.bpe_vocab
bpe_vocab_trg=${train_bpe}.$trg.bpe_vocab


#####################################################################
# (2) train.sh initial settings (modify if needed)                  #
#####################################################################

# Model architecture
num_src_embed=512 #[64,1024]
num_trg_embed=512 #[64,1024]
# num_embed="${num_src_embed}:${num_trg_embed}"
rnn_num_hidden=1024 #[64,2048]
num_layers=1 #[1,2]

# Training configuration
max_seq_len="100:100" # do not change
num_src_words=5000 #[2000,80000]
num_trg_words=5000 #[2000,80000]
# num_words="${num_src_words}:${num_trg_words}"
word_src_count=1 #[1,2]
word_trg_count=1 #[1,2]
# word_min_count="${word_src_count}:${word_trg_count}"
batch_size=64 #[16,256]

# dropout: [0,0.8]
embed_src_dropout=.0
embed_trg_dropout=.0
# embed_dropout="${embed_src_dropout}:${embed_trg_dropout}"
rnn_encoder_dropout_outputs=.0
rnn_decoder_dropout_outputs=.0
# rnn_dropout_inputs="${rnn_encoder_dropout_outputs}:${rnn_decoder_dropout_outputs}"
rnn_encoder_dropout_states=.0
rnn_decoder_dropout_states=.0
# rnn_dropout_states="${rnn_encoder_dropout_states}:${rnn_decoder_dropout_states}"
rnn_decoder_hidden_dropout=.0

initial_learning_rate=0.003 #[0.0001,0.1]

# tune manually (outside cmaes)
optimizer="adam" # adam, sgd, rmsprop
rnn_cell_type="lstm" # lstm, gru(speed)
attention_type="dot" # dot, bilinear, dot_scaled, coverage
beam_size=5 #[1,16]


# Logging and stopping condition
min_num_epochs=15
max_num_epochs=15
checkpoint_frequency=1000
keep_last_params=1


#####################################################################
# (3) auto-tuning settings (modify if needed)                       #
#####################################################################

# hyperparameters to be tuned with initial value
params="{'num_src_embed':512,"\
"'num_trg_embed':512,"\
"'rnn_num_hidden':1024,"\
"'num_layers':1,"\
"'num_src_words':5000,"\
"'num_trg_words':5000,"\
"'word_src_count':1,"\
"'word_trg_count':1,"\
"'batch_size':64,"\
"'embed_src_dropout':.0,"\
"'embed_trg_dropout':.0,"\
"'rnn_encoder_dropout_outputs':.0,"\
"'rnn_decoder_dropout_outputs':.0,"\
"'rnn_encoder_dropout_states':.0,"\
"'rnn_decoder_dropout_states':.0,"\
"'rnn_decoder_hidden_dropout':.0,"\
"'initial_learning_rate':0.003"\
"}"

# number of optimization objectives
# 1 for bleu, 2 for bleu and time
n_object=1

# current generation
n_generation=0

# number of generation 
generation=10

# number of population
population=30

# path to checkpoint folder
checkpoint_path=$autotunedir/checkpoints/

# checkpoint files
checkpoint="${checkpoint_path}es_G%s.pkl"

# parent directory of current generation folder
generation_dir=$autotunedir/

# parent directory of previous generation folder
prev_generation_dir=$autotunedir/

# command for python interpreter
py_cmd=python3
